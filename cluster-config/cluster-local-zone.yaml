apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eks-demo-local-zone
  region: us-east-1
  version: "1.30"
  # tags:
  #   karpenter.sh/discovery: eks-demo-local-zone

availabilityZones:
- us-east-1a
- us-east-1b

vpc:
  cidr: 192.168.0.0/16
  clusterEndpoints:
    privateAccess: true
    publicAccess: true
  manageSharedNodeSecurityGroupRules: true
  nat:
    gateway: HighlyAvailable
  publicAccessCIDRs: # you should configured a proper CIDR list here
  - 0.0.0.0/0
  # id: "vpc-XXX"
  # securityGroup: "sg-XXX"
  # subnets:
  #   public:
  #     localzone-1:
  #       az: "us-east-1-msp-1a"
  #       id: "subnet-XXX"
  #     localzone-2:
  #       az: "us-east-1-msp-1a"
  #       id: "subnet-XXX"

accessConfig:
  authenticationMode: API_AND_CONFIG_MAP
  bootstrapClusterCreatorAdminPermissions: true

iam:
  withOIDC: true

nodeGroups:
  - name: ng-lz1
    amiFamily: AmazonLinux2023
    minSize: 2
    maxSize: 5
    desiredCapacity: 2
    volumeSize: 30
    volumeType: gp2
    instanceType: t3.small # Local Zone have no "t3a", https://aws.amazon.com/about-aws/global-infrastructure/localzones/features/
    ssh:
      allow: true
      publicKeyName: "sshKeyName"
    subnets:
      - localzone-1
      - localzone-2

addons:
  - name: kube-proxy
    version: latest
  - name: vpc-cni
    version: latest
    useDefaultPodIdentityAssociations: true
  - name: coredns
    version: latest
  - name: eks-pod-identity-agent
    version: latest

cloudWatch:
  # ref: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html
  clusterLogging:
    logRetentionInDays: 90
    enableTypes:
    - "api"
    - "audit"
    - "authenticator"
    - "controllerManager"
    - "scheduler"
